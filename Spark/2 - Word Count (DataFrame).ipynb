{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count using DataFrames\n",
    "\n",
    "Spark DataFrames, a relatively new feature, provide a higher level interface similar to pandas DataFrames that abstract away much of the detail when using Spark. This exercise implements the previous word count with RDDs example, using DataFrames.\n",
    "\n",
    "We will cover:\n",
    "\n",
    "1. Creating DataFrames\n",
    "2. Counting with `.groupBy()` and `.count()`\n",
    "3. Finding unique words and a mean value\n",
    "4. Applying word count to a file\n",
    "\n",
    "Note that, for reference, you can look up the details of the relevant methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import types\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Creating a Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, we will explore creating a DataFrame with `createDataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1a) Create a base DataFrame\n",
    "\n",
    "We'll start by generating a base DataFrame by using a pandas DataFrame and the `spark.createDataFrame` method.  Then we'll print out the type of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wordsPandasDF = pd.DataFrame({'word': ['cat', 'elephant', 'rat', 'rat', 'cat']})\n",
    "wordsDF = spark.createDataFrame(wordsPandasDF)\n",
    "\n",
    "# Print out the type of wordsRDD\n",
    "print(type(wordsDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to a pandas DataFrame for pretty display\n",
    "wordsDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1b) Pluralize and test\n",
    "\n",
    "Like in the previous exercise, let's create a function to pluralize a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way of completing the function\n",
    "def makePlural(word):\n",
    "    return word + 's'\n",
    "\n",
    "print(makePlural('cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the testing code and check to see if your answer is correct\n",
    "# If incorrect it will report back '1 test failed' for each failed test\n",
    "# Make sure to rerun any cell you change before trying the test again\n",
    "from test_helper import Test\n",
    "# TEST Pluralize and test (1b)\n",
    "Test.assertEquals(makePlural('rat'), 'rats', 'incorrect result: makePlural does not add an s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1c) Apply `makePlural` to the base DataFrame\n",
    "\n",
    "In order to apply `makePlural` to a column from a Spark DataFrame, we must first create a Spark function. [pyspark.sql.functions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) provides a number of built in functions you can use, as well as [udf()](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf), which allows you to create user defined functions.\n",
    "\n",
    "`udf()` takes as its arguments both the python function you want to use and a Spark type, which are defined in the [pyspark.sql.types](http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.types) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import types\n",
    "\n",
    "# Create a Spark User Defined Function\n",
    "makePluralUDF = func.udf(makePlural, types.StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use `.select()` on a DataFrame to return a new DataFrame with the modified column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new DataFrame with a pluralized column\n",
    "pluralDF = wordsDF.select(makePluralUDF('word'))\n",
    "pluralDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Apply makePlural to the base RDD(1c)\n",
    "Test.assertEquals(list(pluralDF.toPandas()['makePlural(word)']),\n",
    "                  ['cats', 'elephants', 'rats', 'rats', 'cats'],\n",
    "                  'incorrect values for pluralRDD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1d) Make a UDF from a `lambda` function\n",
    "\n",
    "You can also create a UDF without first doing a `def` using a Python lambda function. Create the same UDF as above using a lambda function and apply it to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Hint: Don't forget to pass in both the lambda function and the output type, e.g. types.StringType()\n",
    "makePluralLambdaUDF = func.udf(<FILL IN>)\n",
    "\n",
    "pluralLambdaDF = wordsDF.select(makePluralLambdaUDF('word'))\n",
    "pluralLambdaDF.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note as well that you can control the name of the column in the output DataFrame using `.alias()` on the UDF call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pluralLambdaDF = wordsDF.select(makePluralLambdaUDF('word').alias('plural'))\n",
    "pluralLambdaDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Pass a lambda function to map (1d)\n",
    "Test.assertEquals(list(pluralLambdaDF.toPandas()['plural']),\n",
    "                  ['cats', 'elephants', 'rats', 'rats', 'cats'],\n",
    "                  'incorrect values for pluralLambdaRDD (1d)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1e) Length of each word\n",
    "\n",
    "Now construct a UDF to return the number of characters in each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "# Hint: Don't forget to pass in both a Python function and the output type, e.g. types.StringType()\n",
    "lenUDF = func.udf(<FILL IN>)\n",
    "\n",
    "lengthsDF = pluralLambdaDF.select('*', lenUDF('plural'))\n",
    "lengthsDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Length of each word (1e)\n",
    "Test.assertEquals(list(lengthsDF.toPandas()['len(plural)']),\n",
    "                  [4, 9, 4, 4, 4],\n",
    "                  'incorrect values for pluralLengths')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Counting with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) `.groupBy()`\n",
    "\n",
    "When using DataFrames, we have a more abstract interface for performing complex operations on data. The `.groupBy()` method on a DataFrame returns a [GroupedData](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData) object, which supports a number of actions on grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groupedWords = wordsDF.groupBy('word')\n",
    "help(groupedWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2b) `.groupBy().count()`\n",
    "\n",
    "In our case, where we have a single column of strings, only the `.count()` method really makes sense. This returns a new DataFrame with two columns, one with the words and one with the number of times they appeared in the source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCounts = wordsDF.groupBy('word').count()\n",
    "wordCounts.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST word counts (2b)\n",
    "wordCountsPandasDF = wordCounts.toPandas()\n",
    "Test.assertEquals(sorted(zip(wordCountsPandasDF['word'], wordCountsPandasDF['count'])),\n",
    "                  [('cat', 2), ('elephant', 1), ('rat', 2)],\n",
    "                  'incorrect value for wordCountsCollected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Finding unique words and a mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3a) Unique words\n",
    "\n",
    "Calculate the number of unique words in `wordsDF`.  You can use other DataFrames that you have already created to make this easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "numUniqueWords = <FILL IN>\n",
    "print(numUniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Unique words (3a)\n",
    "Test.assertEquals(numUniqueWords, 3, 'incorrect count of numUniqueWords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3b) Mean word count\n",
    "\n",
    "Find the mean number of occurrences of each word in `wordCounts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "averageDF = <FILL IN>\n",
    "\n",
    "# Collect DataFrame and get first value out of it\n",
    "average = np.array(averageDF.toPandas())[0, 0]\n",
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Mean using reduce (3b)\n",
    "Test.assertEqualsTol(average, 1.6666667, 0.01, 'incorrect value of average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Apply word count to a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will finish developing our word count application.  We'll have to build the `wordCount` function, deal with real world problems like capitalization and punctuation, load in our data source, and compute the word count on the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4a) `wordCount` function\n",
    "\n",
    "First, define a function for word counting.  This function should take in a DataFrame that is a list of words like `wordListDF` and return a DataFrame that has all of the words and their associated counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def wordCount(wordListDF):\n",
    "    \"\"\"Creates a DataFrame with word counts from a DataFrame of words.\n",
    "\n",
    "    Args:\n",
    "        wordListDF (DataFrame): A DataFrame consisting of words.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A Spark DataFrame with word, count columns.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "wordCount(wordsDF).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST wordCount function (4a)\n",
    "wordCountsPandasDF = wordCount(wordsDF).toPandas()\n",
    "Test.assertEquals(sorted(zip(wordCountsPandasDF['word'], wordCountsPandasDF['count'])),\n",
    "                  [('cat', 2), ('elephant', 1), ('rat', 2)],\n",
    "                  'incorrect value for wordCountsCollected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4b) Capitalization and punctuation\n",
    "\n",
    "Here we defined the `removePunctuation` function from the previous exercise to preprocess text to cleaned words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "import re\n",
    "\n",
    "def removePunctuation(text):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        text (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return <FILL IN>\n",
    "\n",
    "print(removePunctuation('Hi, you!'))\n",
    "print(removePunctuation(' No under_score!'))\n",
    "print(removePunctuation(\" The Elephant's 4 cats. \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Capitalization and punctuation (4b)\n",
    "Test.assertEquals(removePunctuation(\" The Elephant's 4 cats. \"),\n",
    "                  'the elephants 4 cats',\n",
    "                  'incorrect definition for removePunctuation function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4c) Load a text file\n",
    "\n",
    "For the next part, we will use the [Complete Works of William Shakespeare](http://www.gutenberg.org/ebooks/100) from [Project Gutenberg](http://www.gutenberg.org/wiki/Main_Page). To convert a text file into a DataFrame, we use the `SparkContext.textFile()` method. We also apply the recently defined `removePunctuation()` function using a `map()` transformation to strip out the punctuation and change all text to lowercase.  Since the file is large we use `take(15)`, so that we only print 15 lines.\n",
    "\n",
    "Let's start by fetching the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm shakespeare.txt*\n",
    "!wget https://s3-eu-west-1.amazonaws.com/asi-training-data/spark/shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"shakespeare.txt\"\n",
    "\n",
    "# Create a UDF for removePunctuation\n",
    "removePunctuationUDF = func.udf(removePunctuation, types.StringType())\n",
    "\n",
    "# Read the data from file and apply removePunctuation\n",
    "shakespeareDF = spark.read.text(filename).select(removePunctuationUDF('value').alias('line'))\n",
    "\n",
    "shakespeareDF.toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4d) Words from lines \n",
    "\n",
    "Before we can use the `wordCount()` function, we have to address two issues with the format of the DataFrame:\n",
    "\n",
    "* The first issue is that  that we need to split each line by its spaces.\n",
    "* The second issue is we need to filter out empty lines.\n",
    " \n",
    "Apply a transformation that will split each element of the DataFrame by its spaces. For each element of the DataFrame, you should apply Python's string [split()](https://docs.python.org/3/library/stdtypes.html#str.split) function. You might think that applying a UDF to a column directly is the way to do this, but think about what the result of the `split()` function will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "shakespeareWordsDF = (shakespeareDF\n",
    "                      .select(<FILL IN>).alias('word')))\n",
    "\n",
    "shakespeareWordsDF.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Words from lines (4d)\n",
    "# This test allows for leading spaces to be removed either before or after\n",
    "# punctuation is removed.\n",
    "Test.assertTrue(shakespeareWordsDF.count() in [927631, 928908],\n",
    "                'incorrect number of words in shakespeareWordsDF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4e) Remove empty elements\n",
    "\n",
    "The next step is to filter out the empty elements.  Remove all entries where the word is `''`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "shakeWordsDF = shakespeareWordsDF.filter(<FILL IN>)\n",
    "shakeWordCount = shakeWordsDF.count()\n",
    "print(shakeWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Remove empty elements (4e)\n",
    "Test.assertEquals(shakeWordCount, 882996, 'incorrect value for shakeWordCount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4f) Count the words\n",
    "\n",
    "We now have an RDD that is only words.  Next, let's apply the `wordCount()` function to produce a list of word counts. We can view the top 15 words by using the `takeOrdered()` action; however, since the elements of the RDD are pairs, we need a custom sort function that sorts using the value part of the pair.\n",
    "\n",
    "You'll notice that many of the words are common English words. These are called stopwords. In a later lab, we will see how to eliminate them from the results.\n",
    "\n",
    "Use the `wordCount()` function and `takeOrdered()` to obtain the fifteen most common words and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "top15WordsAndCounts = <FILL IN>\n",
    "\n",
    "for row in top15WordsAndCounts:\n",
    "    print('{0}: {1}'.format(*row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Count the words (4f)\n",
    "Test.assertEquals(top15WordsAndCounts,\n",
    "                  [(u'the', 27361), (u'and', 26028), (u'i', 20681), (u'to', 19150), (u'of', 17463),\n",
    "                   (u'a', 14593), (u'you', 13615), (u'my', 12481), (u'in', 10956), (u'that', 10890),\n",
    "                   (u'is', 9134), (u'not', 8497), (u'with', 7771), (u'me', 7769), (u'it', 7678)],\n",
    "                  'incorrect value for top15WordsAndCounts')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
