{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will see what the MNIST dataset is, and how to perform classification on it using Neural Networks. The Python library we are going to use is PyBrain.\n",
    "\n",
    "For a more detailed description of the problem, have a look at \n",
    "http://martin-thoma.com/classify-mnist-with-pybrain/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from numpy import zeros, uint8, ravel\n",
    "\n",
    "import pylab as plt\n",
    "from pylab import imshow, show, cm\n",
    "\n",
    "from pybrain.datasets import ClassificationDataSet\n",
    "from pybrain.utilities import percentError\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.structure.modules import SoftmaxLayer\n",
    "\n",
    "import os.path\n",
    "import numpy as np\n",
    "import idx2numpy\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with the MNIST (http://yann.lecun.com/exdb/mnist/) dataset of handwritten digits. In this case the data are small images, we can define a function to visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View a single image, possibly with the associated label\n",
    "def view_image(image, label=\"\"):\n",
    "    print(\"Label: %s\" % label)\n",
    "    imshow(image, cmap=cm.gray)\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to open our data. They come already split in training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get test set\n",
    "\n",
    "images = gzip.open('t10k-images-idx3-ubyte.gz', 'rb')\n",
    "labels = gzip.open('t10k-labels-idx1-ubyte.gz', 'rb')\n",
    "\n",
    "# sample size\n",
    "rows=28\n",
    "cols=28\n",
    "\n",
    "# build a dictionary for the data\n",
    "testing = {'x':idx2numpy.convert_from_file(images), 'y':idx2numpy.convert_from_file(labels), 'rows':rows, 'cols':cols}\n",
    "\n",
    "print(\"Number of test samples: %i\" % len(testing['x']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the same procedure for training samples. For time reasons, we won't use all the training data (60K) but only a subset (1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ntrain = 1000\n",
    "\n",
    "images = gzip.open('train-images-idx3-ubyte.gz', 'rb')\n",
    "labels = gzip.open('train-labels-idx1-ubyte.gz', 'rb')\n",
    "\n",
    "rows=28\n",
    "cols=28\n",
    "training = {'x':idx2numpy.convert_from_file(images), 'y':idx2numpy.convert_from_file(labels), 'rows':rows, 'cols':cols}\n",
    "\n",
    "idx = np.random.permutation(xrange(len(training['x'])))\n",
    "\n",
    "training['x'] = training['x'][idx[:Ntrain], :, :]\n",
    "training['y'] = training['y'][idx[:Ntrain]]\n",
    "\n",
    "    \n",
    "print(\"Number of training samples: %i\" % len(training['x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize some of the samples - change the index to see different digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 109\n",
    "view_image(training['x'][index], label=training['y'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 12))\n",
    "nrows = 2\n",
    "ncols = 4\n",
    "idx = np.random.permutation(xrange(len(training['x'])))\n",
    "k = 0\n",
    "for i in xrange(2):\n",
    "    for j in xrange(4):\n",
    "        plt.subplot(nrows, ncols, k)\n",
    "        plt.imshow(training['x'][idx[k]], cmap=cm.gray)\n",
    "        k+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first have a look at the different parameters we need to build the network and perform the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# number of neurons in the hidden layer, that is, what extracts features/aspects of the input\n",
    "hidden_neurons = 200\n",
    "\n",
    "# number of iterations over the dataset to train the network\n",
    "epochs = 10\n",
    "\n",
    "# how much an updating step influences the current value of the weights\n",
    "learning_rate = 0.01\n",
    "\n",
    "# how fast the learning rate goes to zero\n",
    "lrdecay = 1\n",
    "\n",
    "# how much weight are reduced after each update\n",
    "weightdecay = 0.01\n",
    "\n",
    "# adds a fraction of the previous weight update to the current one\n",
    "momentum = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting dataset for PyBrain usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how many features?\n",
    "input_features = testing['rows'] * testing['cols']\n",
    "print(\"Input features: %i\" % input_features)\n",
    "\n",
    "#MNIST has 10 classes (digits 0 to 9)\n",
    "classes = 10\n",
    "# build datasets with PyBrain\n",
    "train_data = ClassificationDataSet(input_features, 1, nb_classes=classes)\n",
    "test_data = ClassificationDataSet(input_features, 1, nb_classes=classes)\n",
    "\n",
    "# add samples to training and test set\n",
    "for i in range(len(testing['x'])):\n",
    "    test_data.addSample(ravel(testing['x'][i]), [testing['y'][i]])\n",
    "for i in range(len(training['x'])):\n",
    "    train_data.addSample(ravel(training['x'][i]), [training['y'][i]])\n",
    "\n",
    "# turns into convenient data structure for PyBrain\n",
    "train_data._convertToOneOfMany()\n",
    "test_data._convertToOneOfMany()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the network and performing classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# building the network!\n",
    "net = buildNetwork(train_data.indim, hidden_neurons, train_data.outdim, outclass=SoftmaxLayer)\n",
    "\n",
    "# backpropagation trainer\n",
    "trainer = BackpropTrainer(net, dataset=train_data, momentum=momentum,\n",
    "                              verbose=False, weightdecay=weightdecay,\n",
    "                              learningrate=learning_rate,\n",
    "                              lrdecay=lrdecay)\n",
    "\n",
    "# training and testing the network\n",
    "for i in range(epochs):\n",
    "    trainer.trainEpochs(1)\n",
    "    train_res = percentError(trainer.testOnClassData(),\n",
    "                                 train_data['class'])\n",
    "    test_res = percentError(trainer.testOnClassData(\n",
    "                                 dataset=test_data), test_data['class'])\n",
    "\n",
    "    print \"epoch: \", trainer.totalepochs\n",
    "    print \"train error: \", train_res\n",
    "    print \"test error: \", test_res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think would happen if we used more training data? \n",
    "\n",
    "If you have time, build a cross validation step to see how much the error changes with different chunks of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
