{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import sklearn\n",
    "import operator\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import feature_selection\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "%matplotlib inline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Use the same data as the KNN notebook. Read and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('german.csv',header=None)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split to train and test data\n",
    "train, test = train_test_split(data, train_size = 0.7)\n",
    "\n",
    "train_X = np.array(train)[:, :24]\n",
    "train_Y = np.array(train)[:,24]\n",
    "\n",
    "test_X = np.array(test)[:, :24]\n",
    "test_Y = np.array(test)[:,24]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can redefine the accuracy measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAccuracy(testSet, predictions):\n",
    "    correct = 0\n",
    "    for i in range(len(testSet)):\n",
    "        #If the label of the testSet and the prediction are the same add one.\n",
    "        if testSet[i] == predictions[i]:\n",
    "            correct += 1\n",
    "    return (float(correct)/float(len(testSet))) * 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "Let's start with a linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the linear model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train it with the training data\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "# Predict the new unseen test dataset\n",
    "predictions = clf.predict(test_X)\n",
    "\n",
    "# Calculate the overall accuracy of correctly classified instances\n",
    "accuracy = getAccuracy(test_Y, predictions)\n",
    "print('Accuracy: ', accuracy, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we auto-scale (standardise) our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We start by constructing our scaler \n",
    "scaler  = preprocessing.StandardScaler().fit(train_X.astype(float))\n",
    "train_Xscaled = scaler.transform(train_X.astype(float))\n",
    "\n",
    "# The scaler instance is used on our test data to transform it the same way it did on the training set:\n",
    "test_Xscaled = scaler.transform(test_X.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(train_Xscaled, train_Y)\n",
    "\n",
    "predictions = clf.predict(test_Xscaled)\n",
    "\n",
    "accuracy = getAccuracy(test_Y, predictions)\n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same steps with the rbf kernel.\n",
    "\n",
    "We begin once more with the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Default values for C and gamma. \n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=1.0, gamma=0.0)\n",
    "clf.fit(train_X, train_Y)\n",
    "\n",
    "predictions = clf.predict(test_X)\n",
    "\n",
    "accuracy = getAccuracy(test_Y, predictions)\n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while with scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='rbf', C=1.0, gamma=0.0)\n",
    "clf.fit(train_Xscaled, train_Y)\n",
    "\n",
    "predictions = clf.predict(test_Xscaled)\n",
    "\n",
    "accuracy = getAccuracy(test_Y, predictions)\n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the RBF kernel hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if now we optimise the hyperparameters using a coarse tuning? (this may take a bit of time to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gamma and Cost allowed values\n",
    "gamma_range = 2. ** np.arange(-15, 5, step=2)\n",
    "C_range     = 2. ** np.arange(-5, 15, step=2)\n",
    "\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=KFold(n=train_Xscaled.shape[0], n_folds=5))\n",
    "\n",
    "grid.fit(train_Xscaled, train_Y)\n",
    "\n",
    "print(\"The best parameters are: gamma=\", np.log2(grid.best_params_['gamma']), \n",
    "      \" and Cost=\", np.log2(grid.best_params_['C']))\n",
    "bestG = np.log2(grid.best_params_['gamma']);\n",
    "bestC = np.log2(grid.best_params_['C']);\n",
    "\n",
    "# plot the scores of the grid\n",
    "# grid_scores_ contains parameter settings and scores\n",
    "score_dict = grid.grid_scores_\n",
    "\n",
    "# We extract just the scores\n",
    "scores = [x[1] for x in score_dict]\n",
    "scores = np.array(scores).reshape(len(C_range), len(gamma_range))\n",
    "\n",
    "\n",
    "# Make a heatmap with the performance\n",
    "pl.figure(figsize=(10, 6))\n",
    "pl.subplots_adjust(left=0.15, right=0.95, bottom=0.15, top=0.95)\n",
    "v = np.linspace(0.8, 1.0, 15, endpoint=True)\n",
    "pl.imshow(scores, interpolation='nearest', origin='higher', cmap=pl.cm.get_cmap('jet_r'))\n",
    "pl.xlabel('gamma (log2)')\n",
    "pl.ylabel('Cost (log2)')\n",
    "cbar = pl.colorbar()\n",
    "pl.xticks(np.arange(len(gamma_range)), np.log2(gamma_range))\n",
    "pl.yticks(np.arange(len(C_range)), np.log2(C_range))\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the selected parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='rbf', C=grid.best_params_['C'], gamma=grid.best_params_['gamma'])\n",
    "clf.fit(train_Xscaled, train_Y)\n",
    "\n",
    "predictions = clf.predict(test_Xscaled) \n",
    "\n",
    "accuracy = getAccuracy(test_Y, predictions)\n",
    "print('Accuracy: ' + repr(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which kernel gave the best accuracy? In which case?\n",
    "\n",
    "You can repeat the parameters tuning with other kernels.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
